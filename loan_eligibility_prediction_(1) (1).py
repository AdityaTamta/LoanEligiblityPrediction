# -*- coding: utf-8 -*-
"""Loan_Eligibility_Prediction (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16rJN3LvkDEEy_WYY2rJPJ0on9fPzwUsW
"""

import pandas as pd
import numpy as np
from sklearn import svm
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv("loan.csv")

df.head()

df.info()

df.isnull().sum()

df['loanAmount_log']=np.log(df['LoanAmount'])
df['loanAmount_log'].hist(bins=20)



df.isnull().sum()

df['TotalIncome']=df['ApplicantIncome']+df['CoapplicantIncome']
df['TotalIncome_log']=np.log(df['TotalIncome'])
df['TotalIncome_log'].hist(bins=20)

df['Gender'].fillna(df['Gender'].mode()[0], inplace = True)
  df['Married'].fillna(df['Married'].mode()[0], inplace = True)
  df['Self_Employed'].fillna(df['Self_Employed'].mode()[0], inplace = True)
  df['Dependents'].fillna(df['Dependents'].mode()[0], inplace = True)
  df.LoanAmount=df.LoanAmount.fillna(df.LoanAmount.mean())
  df.loanAmount_log=df.loanAmount_log.fillna(df.loanAmount_log.mean())
  df['Loan_Amount_Term'].fillna(df['Loan_Amount_Term'].mode()[0], inplace = True)
  df['Credit_History'].fillna(df['Credit_History'].mode()[0], inplace = True)
  df.isnull().sum()

x=df.iloc[:,np.r_[1:5,9:11,13:15]].values
y=df.iloc[:,12].values
x

y

print("percent of missing gender is  %2f%%"%((df['Gender'].isnull().sum()/df.shape[0])*100))

print("number of people who take loan as groupwise gender:")
print(df['Gender'].value_counts())
sns.countplot(x='Gender',data=df,palette='Set1')

print("number of people who take loan as marital status:")
print(df['Married'].value_counts())
sns.countplot(x='Married',data=df,palette='Set1')

print("number of people who take loan as dependents:")
print(df['Dependents'].value_counts())
sns.countplot(x='Dependents',data=df,palette='Set1')

print("number of people who take loan as Self Employed:")
print(df['Self_Employed'].value_counts())
sns.countplot(x='Self_Employed',data=df,palette='Set1')

print("number of people who take loan as LoanAmount:")
print(df['LoanAmount'].value_counts())
sns.countplot(x='LoanAmount',data=df,palette='Set1')

print("number of people who take loan as Credit History:")
print(df['Credit_History'].value_counts())
sns.countplot(x='Credit_History',data=df,palette='Set1')

# To check the class imbalance
loan_status_counts = df['Loan_Status'].value_counts(normalize=True) * 100
loan_status_counts

from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split


X = df.drop(columns=["Loan_ID", "Loan_Status"])
y = df["Loan_Status"]

X = pd.get_dummies(X, drop_first=True)

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# Apply SMOTE on training data
smote = SMOTE(random_state=42)
X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)

balanced_counts = pd.Series(y_train_balanced).value_counts()


balanced_proportions = pd.Series(y_train_balanced).value_counts(normalize=True) * 100

print("Class distribution after SMOTE:\n", balanced_counts)
print("\nClass proportion after SMOTE (%):\n", balanced_proportions)

from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
X_train = ss.fit_transform(X_train)
x_test = ss.transform(X_test)

from sklearn.naive_bayes import GaussianNB
model = GaussianNB()
model.fit(X_train_balanced, y_train_balanced)

y_pred = model.predict(X_test)

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
accuracy = accuracy_score(y_test, y_pred)
print(accuracy)

print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))
print("-" * 45)

print("Classification Report:")
print(classification_report(y_test, y_pred))

from sklearn.ensemble import RandomForestClassifier
rf_clf = RandomForestClassifier(random_state=42)
rf_clf.fit(X_train_balanced, y_train_balanced)

y_pred_rf = rf_clf.predict(X_test)

accuracy_rf = accuracy_score(y_test, y_pred_rf)
print(f"Accuracy of Random Forest model: {accuracy_rf}")
print("-" * 60)

# Print the confusion matrix
print("Confusion Matrix (Random Forest):")
print(confusion_matrix(y_test, y_pred_rf))


# Print the classification report for a detailed performance breakdown
print("Classification Report (Random Forest):")
print(classification_report(y_test, y_pred_rf))

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

dt = DecisionTreeClassifier(random_state=42)

dt.fit(X_train_balanced, y_train_balanced)

y_pred_dt = dt.predict(X_test)

accuracy_dt = accuracy_score(y_test, y_pred_dt)
print(f"Accuracy of Decision Tree model: {accuracy_dt}")
print("-" * 60)

# Print the confusion matrix
print("Confusion Matrix (Decision Tree):")
print(confusion_matrix(y_test, y_pred_dt))


# Print the classification report for a detailed performance breakdown
print("Classification Report (Decision Tree):")
print(classification_report(y_test, y_pred_dt))

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train_balanced, y_train_balanced)

y_pred_knn = knn.predict(X_test)

accuracy_knn = accuracy_score(y_test, y_pred_knn)
print(f"Accuracy of K-Nearest Neighbors model: {accuracy_knn:.4f}")
print("-" * 60)

import xgboost as xgb
xgb_clf = xgb.XGBClassifier(
    n_estimators=100,
    learning_rate=0.1,
    use_label_encoder=False,
    eval_metric='logloss',
    random_state=42
)

label_mapping = {'Y': 1, 'N': 0}
y_train_xgb = pd.Series(y_train_balanced).map(label_mapping).values
y_test_xgb = pd.Series(y_test).map(label_mapping).values

xgb_clf.fit(X_train_balanced, y_train_xgb)

y_pred_xgb = xgb_clf.predict(X_test)
print("-" * 60)
print("XGBoost Model Performance Evaluation:")
accuracy_xgb = accuracy_score(y_test_xgb, y_pred_xgb)
print(f"Accuracy of XGBoost model: {accuracy_xgb:.4f}")
print("-" * 60)
print("Confusion Matrix (XGBoost):")
print(confusion_matrix(y_test_xgb, y_pred_xgb))
print("-" * 60)
print("Classification Report (XGBoost):")
print(classification_report(y_test_xgb, y_pred_xgb, target_names=['N', 'Y']))

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
label_mapping = {'Y': 1, 'N': 0}
y_train_nn = pd.Series(y_train_balanced).map(label_mapping).values
y_test_nn = pd.Series(y_test).map(label_mapping).values
model = Sequential([

    Dense(64, activation='relu', input_shape=(X_train_balanced.shape[1],)),
    Dropout(0.3),
    Dense(32, activation='relu'),
    Dropout(0.2),
    Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
history = model.fit(
    X_train_balanced,
    y_train_nn,
    validation_data=(X_test, y_test_nn),
    epochs=20,
    batch_size=32,
    verbose=1
)
print("Training complete.")
print("-" * 60)
y_pred_prob_nn = model.predict(X_test)
y_pred_nn = (y_pred_prob_nn > 0.5).astype("int32")
print("Keras Model Performance Evaluation:")
print("Confusion Matrix (Keras NN):")
print(confusion_matrix(y_test_nn, y_pred_nn))
print("-" * 60)
print("Classification Report (Keras NN):")
print(classification_report(y_test_nn, y_pred_nn, target_names=['N', 'Y']))